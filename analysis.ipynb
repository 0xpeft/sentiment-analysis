{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chardet\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the folder path\n",
    "folder_path = \"/Users/thyag/Downloads/intershala assignment/StopWords\"\n",
    "\n",
    "# Initialize an empty list to store all words\n",
    "all_words = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is a text file\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Open the file in binary mode\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            raw_data = file.read()\n",
    "            # Detect the encoding\n",
    "            encoding = chardet.detect(raw_data)[\"encoding\"]\n",
    "\n",
    "        # Open the file in text mode with the detected encoding\n",
    "        with open(file_path, \"r\", encoding=encoding) as file:\n",
    "            # Read the contents of the file\n",
    "            contents = file.read()\n",
    "            # Split the contents into words\n",
    "            words = contents.split()\n",
    "            # Add the words to the all_words list\n",
    "            all_words.extend(words)\n",
    "\n",
    "stopwords= [word.lower() for word in all_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the positive and negative word lists\n",
    "positive_word_list_path = \"/Users/thyag/Downloads/intershala assignment/MasterDictionary/positive-words.txt\"\n",
    "negative_word_list_path = \"/Users/thyag/Downloads/intershala assignment/MasterDictionary/negative-words.txt\"\n",
    "\n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "\n",
    "# Detect encoding for positive word list\n",
    "with open(positive_word_list_path, \"rb\") as file:\n",
    "    raw_data = file.read()\n",
    "    encoding = chardet.detect(raw_data)[\"encoding\"]\n",
    "\n",
    "with open(positive_word_list_path, \"r\", encoding=encoding) as file:\n",
    "    for line in file:\n",
    "        word = line.strip().lower()\n",
    "        positive_words.add(word)\n",
    "\n",
    "# Detect encoding for negative word list\n",
    "with open(negative_word_list_path, \"rb\") as file:\n",
    "    raw_data = file.read()\n",
    "    encoding = chardet.detect(raw_data)[\"encoding\"]\n",
    "\n",
    "with open(negative_word_list_path, \"r\", encoding=encoding) as file:\n",
    "    for line in file:\n",
    "        word = line.strip().lower()\n",
    "        negative_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the files\n",
    "directory = \"/Users/thyag/Downloads/intershala assignment/testFiles\"\n",
    "\n",
    "# Initialize an empty list to store file paths\n",
    "file_paths = []\n",
    "\n",
    "# Loop over files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a text file\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Full path to the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Append the file path to the list\n",
    "        file_paths.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    text_file_path = file_paths[i]\n",
    "    with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        # Skip the first line\n",
    "        next(file)\n",
    "        text = file.read()\n",
    "\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Split the text into words\n",
    "    text_words = text.split()\n",
    "    # Remove the stop words from the text words\n",
    "    filtered_words = [word for word in text_words if word not in stopwords]\n",
    "\n",
    "    new_filtered_words = ' '.join(filtered_words)\n",
    "\n",
    "    # Calculate positive and negative word scores\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "\n",
    "    for word in filtered_words:\n",
    "        if word in positive_words:\n",
    "            positive_score += 1\n",
    "        elif word in negative_words:\n",
    "            negative_score -= 1\n",
    "\n",
    "    polarity_score = (positive_score - abs(negative_score)) / (positive_score + abs(negative_score) + 0.000001)\n",
    "    Subjectivity_Score = (positive_score + abs(negative_score))/ ((len(filtered_words)) + 0.000001)\n",
    "\n",
    "\n",
    "    def average_sentence_length(text):\n",
    "        # Count the number of sentences\n",
    "        sentences = text.split('.')\n",
    "        num_sentences = len(sentences)\n",
    "\n",
    "        # Remove empty strings\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "        # Count the number of words\n",
    "        num_words = sum(len(sentence.split()) for sentence in sentences)\n",
    "\n",
    "        # Calculate the average sentence length\n",
    "        if num_sentences > 0:\n",
    "            average_length = num_words / num_sentences\n",
    "        else:\n",
    "            average_length = 0\n",
    "\n",
    "        return average_length\n",
    "\n",
    "\n",
    "    def count_syllables(word):\n",
    "        \"\"\"Count the number of syllables in a word.\"\"\"\n",
    "        vowels = \"aeiouy\"\n",
    "        num_syllables = 0\n",
    "        prev_char_was_vowel = False\n",
    "        word = word.lower()\n",
    "        \n",
    "        # Handle special cases\n",
    "        if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "            word = word[:-2]\n",
    "\n",
    "        for char in word:\n",
    "            if char in vowels:\n",
    "                if not prev_char_was_vowel:\n",
    "                    num_syllables += 1\n",
    "                prev_char_was_vowel = True\n",
    "            else:\n",
    "                prev_char_was_vowel = False\n",
    "\n",
    "        # Adjust for words ending with \"le\"\n",
    "        if word.endswith(\"le\") and len(word) > 2:\n",
    "            num_syllables += 1\n",
    "\n",
    "        # Ensure at least one syllable counted for non-empty words\n",
    "        if len(word) > 0 and num_syllables == 0:\n",
    "            num_syllables = 1\n",
    "\n",
    "        return num_syllables\n",
    "\n",
    "    def percentage_complex_words(text):\n",
    "        # Tokenize the text into words\n",
    "        words = text.split()\n",
    "\n",
    "        # Count the number of complex words\n",
    "        num_complex_words = sum(1 for word in words if count_syllables(word) > 2)\n",
    "\n",
    "        # Calculate the total number of words\n",
    "        num_words = len(words)\n",
    "\n",
    "        # Calculate the percentage of complex words\n",
    "        if num_words > 0:\n",
    "            percentage = (num_complex_words / num_words) * 100\n",
    "        else:\n",
    "            percentage = 0\n",
    "\n",
    "        return percentage\n",
    "\n",
    "    # Example usage:\n",
    "    #text = \"This is a sample sentence with some complex words like 'complicated' and 'entanglement'.\"\n",
    "\n",
    "    def count_syllables(word):\n",
    "        word = word.lower()\n",
    "        vowels = \"aeiouy\"\n",
    "        count = 0\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for index in range(1, len(word)):\n",
    "            if word[index] in vowels and word[index - 1] not in vowels:\n",
    "                count += 1\n",
    "        if word.endswith(\"e\"):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        return count\n",
    "\n",
    "    def is_complex_word(word):\n",
    "        return count_syllables(word) >= 3\n",
    "\n",
    "    def gunning_fog_index(text):\n",
    "        # Split text into sentences\n",
    "        sentences = re.split(r'[.!?]', text)\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "        \n",
    "        # Split text into words\n",
    "        words = re.findall(r'\\w+', text)\n",
    "        \n",
    "        # Count complex words\n",
    "        complex_words = [word for word in words if is_complex_word(word)]\n",
    "        \n",
    "        total_words = len(words)\n",
    "        total_sentences = len(sentences)\n",
    "        total_complex_words = len(complex_words)\n",
    "        \n",
    "        if total_sentences == 0:  # Avoid division by zero\n",
    "            return 0\n",
    "        \n",
    "        gfi = 0.4 * ((total_words / total_sentences) + 100 * (total_complex_words / total_words))\n",
    "        \n",
    "        return gfi\n",
    "\n",
    "    gfi_score = gunning_fog_index(new_filtered_words)\n",
    "\n",
    "\n",
    "\n",
    "    def average_words_per_sentence(text):\n",
    "        # Define a simple sentence tokenizer based on punctuation\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        \n",
    "        # Remove any empty sentences that may result from the split\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "        \n",
    "        total_sentences = len(sentences)\n",
    "        total_words = sum(len(sentence.split()) for sentence in sentences)\n",
    "        \n",
    "        # Calculate the average number of words per sentence\n",
    "        if total_sentences == 0:\n",
    "            return 0\n",
    "        \n",
    "        average = total_words / total_sentences\n",
    "        return average\n",
    "\n",
    "    average = average_words_per_sentence(text)\n",
    "\n",
    "\n",
    "    def count_syllables(word):\n",
    "        \"\"\"Count the number of syllables in a word.\"\"\"\n",
    "        vowels = \"aeiouy\"\n",
    "        num_syllables = 0\n",
    "        prev_char_was_vowel = False\n",
    "        word = word.lower()\n",
    "        \n",
    "        # Handle special cases\n",
    "        if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "            word = word[:-2]\n",
    "\n",
    "        for char in word:\n",
    "            if char in vowels:\n",
    "                if not prev_char_was_vowel:\n",
    "                    num_syllables += 1\n",
    "                prev_char_was_vowel = True\n",
    "            else:\n",
    "                prev_char_was_vowel = False\n",
    "\n",
    "        # Adjust for words ending with \"le\"\n",
    "        if word.endswith(\"le\") and len(word) > 2:\n",
    "            num_syllables += 1\n",
    "\n",
    "        # Ensure at least one syllable counted for non-empty words\n",
    "        if len(word) > 0 and num_syllables == 0:\n",
    "            num_syllables = 1\n",
    "\n",
    "        return num_syllables\n",
    "\n",
    "    def complex_word_count(text):\n",
    "        # Tokenize the text into words\n",
    "        words = text.split()\n",
    "\n",
    "        # Count the number of complex words\n",
    "        num_complex_words = sum(1 for word in words if count_syllables(word) > 2)\n",
    "\n",
    "        return num_complex_words\n",
    "\n",
    "    # Example usage:\n",
    "    #text = \"This is a sample sentence with some complex words like 'complicated' and 'entanglement'.\"\n",
    "\n",
    "    def count_syllables(word):\n",
    "        \"\"\"Count the number of syllables in a word.\"\"\"\n",
    "        vowels = \"aeiouy\"\n",
    "        num_syllables = 0\n",
    "        prev_char_was_vowel = False\n",
    "        word = word.lower()\n",
    "        \n",
    "        # Handle special cases\n",
    "        if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "            word = word[:-2]\n",
    "\n",
    "        for char in word:\n",
    "            if char in vowels:\n",
    "                if not prev_char_was_vowel:\n",
    "                    num_syllables += 1\n",
    "                prev_char_was_vowel = True\n",
    "            else:\n",
    "                prev_char_was_vowel = False\n",
    "\n",
    "        # Adjust for words ending with \"le\"\n",
    "        if word.endswith(\"le\") and len(word) > 2:\n",
    "            num_syllables += 1\n",
    "\n",
    "        # Ensure at least one syllable counted for non-empty words\n",
    "        if len(word) > 0 and num_syllables == 0:\n",
    "            num_syllables = 1\n",
    "\n",
    "        return num_syllables\n",
    "\n",
    "    def syllable_count_per_word(text):\n",
    "        # Tokenize the text into words\n",
    "        words = text.split()\n",
    "\n",
    "        # Count the number of syllables in each word\n",
    "        syllable_counts = [count_syllables(word) for word in words]\n",
    "\n",
    "        return syllable_counts\n",
    "\n",
    "    def average_syllables_per_word(text):\n",
    "        # Get the counts of syllables per word\n",
    "        syllable_counts = syllable_count_per_word(text)\n",
    "        \n",
    "        # Calculate the average syllable per word\n",
    "        total_syllables = sum(syllable_counts)\n",
    "        total_words = len(syllable_counts)\n",
    "        \n",
    "        if total_words > 0:\n",
    "            average_syllables = total_syllables / total_words\n",
    "        else:\n",
    "            average_syllables = 0\n",
    "        \n",
    "        return average_syllables\n",
    "\n",
    "    # Example usage:\n",
    "    #text = \"This is a sample sentence with some complex words like 'complicated' and 'entanglement'.\"\n",
    "\n",
    "    def count_personal_pronouns(text):\n",
    "        # Define the list of personal pronouns\n",
    "        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "        \n",
    "        # Compile regex pattern to match personal pronouns\n",
    "        pattern = r\"\\b(?:{})\\b\".format(\"|\".join(personal_pronouns))\n",
    "\n",
    "        # Find all matches of personal pronouns in the text\n",
    "        matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Exclude matches that are part of other words (e.g., \"US\" as a country)\n",
    "        matches = [match for match in matches if match.lower() != \"us\"]\n",
    "\n",
    "        # Count the occurrences of personal pronouns\n",
    "        count = len(matches)\n",
    "\n",
    "        return count\n",
    "\n",
    "    def average_word_length(text):\n",
    "        # Tokenize the text into words\n",
    "        words = text.split()\n",
    "\n",
    "        # Calculate the total number of characters in all words\n",
    "        total_characters = sum(len(word) for word in words)\n",
    "\n",
    "        # Calculate the total number of words\n",
    "        total_words = len(words)\n",
    "\n",
    "        # Calculate the average word length\n",
    "        if total_words > 0:\n",
    "            average_length = total_characters / total_words\n",
    "        else:\n",
    "            average_length = 0\n",
    "\n",
    "        return average_length\n",
    "\n",
    "    new_data = {\n",
    "        \"POSITIVE SCORE\": [positive_score],\n",
    "        \"NEGATIVE SCORE\": [negative_score],\n",
    "        \"POLARITY SCORE\": [polarity_score],\n",
    "        \"SUBJECTIVITY SCORE\": [Subjectivity_Score],\n",
    "        \"AVG SENTENCE LENGTH\": [average_sentence_length(new_filtered_words)],\n",
    "        \"PERCENTAGE OF COMPLEX WORDS\": [percentage_complex_words(new_filtered_words)],\n",
    "        \"FOG INDEX\": [gfi_score],\n",
    "        \"AVG NUMBER OF WORDS PER SENTENCE\": [average],\n",
    "        \"COMPLEX WORD COUNT\": [complex_word_count(text)],\n",
    "        \"WORD COUNT\": [len(filtered_words)],\n",
    "        \"SYLLABLE PER WORD\": [average_syllables_per_word(text)],\n",
    "        \"PERSONAL PRONOUNS\": [count_personal_pronouns(text)],\n",
    "        \"AVG WORD LENGTH\": [average_word_length(text)]\n",
    "    }\n",
    "\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    # Append the new DataFrame to the existing one\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>-5</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.063529</td>\n",
       "      <td>8.885417</td>\n",
       "      <td>50.705882</td>\n",
       "      <td>22.126485</td>\n",
       "      <td>17.653061</td>\n",
       "      <td>454</td>\n",
       "      <td>850</td>\n",
       "      <td>1.859873</td>\n",
       "      <td>5</td>\n",
       "      <td>5.466705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>-31</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>9.964706</td>\n",
       "      <td>48.996458</td>\n",
       "      <td>21.782689</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>451</td>\n",
       "      <td>847</td>\n",
       "      <td>1.915294</td>\n",
       "      <td>1</td>\n",
       "      <td>5.539412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>32.415254</td>\n",
       "      <td>15.932978</td>\n",
       "      <td>24.395349</td>\n",
       "      <td>170</td>\n",
       "      <td>472</td>\n",
       "      <td>1.678295</td>\n",
       "      <td>3</td>\n",
       "      <td>4.934109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>11.791667</td>\n",
       "      <td>45.714286</td>\n",
       "      <td>20.134452</td>\n",
       "      <td>23.375000</td>\n",
       "      <td>140</td>\n",
       "      <td>280</td>\n",
       "      <td>1.869176</td>\n",
       "      <td>1</td>\n",
       "      <td>5.439068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>-28</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.119086</td>\n",
       "      <td>8.661972</td>\n",
       "      <td>36.704731</td>\n",
       "      <td>16.757334</td>\n",
       "      <td>17.231707</td>\n",
       "      <td>254</td>\n",
       "      <td>613</td>\n",
       "      <td>1.696669</td>\n",
       "      <td>4</td>\n",
       "      <td>5.007087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>33</td>\n",
       "      <td>-24</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.087156</td>\n",
       "      <td>11.135593</td>\n",
       "      <td>46.024465</td>\n",
       "      <td>20.040687</td>\n",
       "      <td>19.603175</td>\n",
       "      <td>323</td>\n",
       "      <td>654</td>\n",
       "      <td>1.908279</td>\n",
       "      <td>7</td>\n",
       "      <td>5.635552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>23</td>\n",
       "      <td>-6</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.057426</td>\n",
       "      <td>6.405063</td>\n",
       "      <td>36.633663</td>\n",
       "      <td>15.013097</td>\n",
       "      <td>15.468354</td>\n",
       "      <td>187</td>\n",
       "      <td>505</td>\n",
       "      <td>1.670762</td>\n",
       "      <td>11</td>\n",
       "      <td>4.708436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>27</td>\n",
       "      <td>-21</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.087591</td>\n",
       "      <td>12.108696</td>\n",
       "      <td>48.357664</td>\n",
       "      <td>19.357823</td>\n",
       "      <td>22.653061</td>\n",
       "      <td>277</td>\n",
       "      <td>548</td>\n",
       "      <td>1.865455</td>\n",
       "      <td>7</td>\n",
       "      <td>5.520909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>27</td>\n",
       "      <td>-21</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.087591</td>\n",
       "      <td>12.108696</td>\n",
       "      <td>48.357664</td>\n",
       "      <td>19.357823</td>\n",
       "      <td>22.653061</td>\n",
       "      <td>277</td>\n",
       "      <td>548</td>\n",
       "      <td>1.865455</td>\n",
       "      <td>7</td>\n",
       "      <td>5.520909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>37</td>\n",
       "      <td>-11</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>11.520000</td>\n",
       "      <td>57.638889</td>\n",
       "      <td>23.553542</td>\n",
       "      <td>24.480000</td>\n",
       "      <td>181</td>\n",
       "      <td>288</td>\n",
       "      <td>1.993464</td>\n",
       "      <td>3</td>\n",
       "      <td>5.553922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0               49              -5        0.814815            0.063529   \n",
       "1               57             -31        0.295455            0.103896   \n",
       "2               13             -10        0.130435            0.048729   \n",
       "3               19              -2        0.809524            0.075000   \n",
       "4               45             -28        0.232877            0.119086   \n",
       "..             ...             ...             ...                 ...   \n",
       "95              33             -24        0.157895            0.087156   \n",
       "96              23              -6        0.586207            0.057426   \n",
       "97              27             -21        0.125000            0.087591   \n",
       "98              27             -21        0.125000            0.087591   \n",
       "99              37             -11        0.541667            0.166667   \n",
       "\n",
       "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0              8.885417                    50.705882  22.126485   \n",
       "1              9.964706                    48.996458  21.782689   \n",
       "2             12.200000                    32.415254  15.932978   \n",
       "3             11.791667                    45.714286  20.134452   \n",
       "4              8.661972                    36.704731  16.757334   \n",
       "..                  ...                          ...        ...   \n",
       "95            11.135593                    46.024465  20.040687   \n",
       "96             6.405063                    36.633663  15.013097   \n",
       "97            12.108696                    48.357664  19.357823   \n",
       "98            12.108696                    48.357664  19.357823   \n",
       "99            11.520000                    57.638889  23.553542   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                          17.653061                 454         850   \n",
       "1                          20.000000                 451         847   \n",
       "2                          24.395349                 170         472   \n",
       "3                          23.375000                 140         280   \n",
       "4                          17.231707                 254         613   \n",
       "..                               ...                 ...         ...   \n",
       "95                         19.603175                 323         654   \n",
       "96                         15.468354                 187         505   \n",
       "97                         22.653061                 277         548   \n",
       "98                         22.653061                 277         548   \n",
       "99                         24.480000                 181         288   \n",
       "\n",
       "    SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0            1.859873                  5         5.466705  \n",
       "1            1.915294                  1         5.539412  \n",
       "2            1.678295                  3         4.934109  \n",
       "3            1.869176                  1         5.439068  \n",
       "4            1.696669                  4         5.007087  \n",
       "..                ...                ...              ...  \n",
       "95           1.908279                  7         5.635552  \n",
       "96           1.670762                 11         4.708436  \n",
       "97           1.865455                  7         5.520909  \n",
       "98           1.865455                  7         5.520909  \n",
       "99           1.993464                  3         5.553922  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load an Excel file into a DataFrame\n",
    "df_index = pd.read_excel('/Users/thyag/Downloads/intershala assignment/Output Data Structure.xlsx')\n",
    "\n",
    "df_index = df_index[['URL_ID', 'URL']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_index.join(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>49</td>\n",
       "      <td>-5</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.063529</td>\n",
       "      <td>8.885417</td>\n",
       "      <td>50.705882</td>\n",
       "      <td>22.126485</td>\n",
       "      <td>17.653061</td>\n",
       "      <td>454</td>\n",
       "      <td>850</td>\n",
       "      <td>1.859873</td>\n",
       "      <td>5</td>\n",
       "      <td>5.466705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>57</td>\n",
       "      <td>-31</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>9.964706</td>\n",
       "      <td>48.996458</td>\n",
       "      <td>21.782689</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>451</td>\n",
       "      <td>847</td>\n",
       "      <td>1.915294</td>\n",
       "      <td>1</td>\n",
       "      <td>5.539412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>13</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>32.415254</td>\n",
       "      <td>15.932978</td>\n",
       "      <td>24.395349</td>\n",
       "      <td>170</td>\n",
       "      <td>472</td>\n",
       "      <td>1.678295</td>\n",
       "      <td>3</td>\n",
       "      <td>4.934109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>19</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>11.791667</td>\n",
       "      <td>45.714286</td>\n",
       "      <td>20.134452</td>\n",
       "      <td>23.375000</td>\n",
       "      <td>140</td>\n",
       "      <td>280</td>\n",
       "      <td>1.869176</td>\n",
       "      <td>1</td>\n",
       "      <td>5.439068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>45</td>\n",
       "      <td>-28</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.119086</td>\n",
       "      <td>8.661972</td>\n",
       "      <td>36.704731</td>\n",
       "      <td>16.757334</td>\n",
       "      <td>17.231707</td>\n",
       "      <td>254</td>\n",
       "      <td>613</td>\n",
       "      <td>1.696669</td>\n",
       "      <td>4</td>\n",
       "      <td>5.007087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>33</td>\n",
       "      <td>-24</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.087156</td>\n",
       "      <td>11.135593</td>\n",
       "      <td>46.024465</td>\n",
       "      <td>20.040687</td>\n",
       "      <td>19.603175</td>\n",
       "      <td>323</td>\n",
       "      <td>654</td>\n",
       "      <td>1.908279</td>\n",
       "      <td>7</td>\n",
       "      <td>5.635552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>23</td>\n",
       "      <td>-6</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.057426</td>\n",
       "      <td>6.405063</td>\n",
       "      <td>36.633663</td>\n",
       "      <td>15.013097</td>\n",
       "      <td>15.468354</td>\n",
       "      <td>187</td>\n",
       "      <td>505</td>\n",
       "      <td>1.670762</td>\n",
       "      <td>11</td>\n",
       "      <td>4.708436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>27</td>\n",
       "      <td>-21</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.087591</td>\n",
       "      <td>12.108696</td>\n",
       "      <td>48.357664</td>\n",
       "      <td>19.357823</td>\n",
       "      <td>22.653061</td>\n",
       "      <td>277</td>\n",
       "      <td>548</td>\n",
       "      <td>1.865455</td>\n",
       "      <td>7</td>\n",
       "      <td>5.520909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>27</td>\n",
       "      <td>-21</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.087591</td>\n",
       "      <td>12.108696</td>\n",
       "      <td>48.357664</td>\n",
       "      <td>19.357823</td>\n",
       "      <td>22.653061</td>\n",
       "      <td>277</td>\n",
       "      <td>548</td>\n",
       "      <td>1.865455</td>\n",
       "      <td>7</td>\n",
       "      <td>5.520909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>37</td>\n",
       "      <td>-11</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>11.520000</td>\n",
       "      <td>57.638889</td>\n",
       "      <td>23.553542</td>\n",
       "      <td>24.480000</td>\n",
       "      <td>181</td>\n",
       "      <td>288</td>\n",
       "      <td>1.993464</td>\n",
       "      <td>3</td>\n",
       "      <td>5.553922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL  \\\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "..              ...                                                ...   \n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
       "\n",
       "    POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0               49              -5        0.814815            0.063529   \n",
       "1               57             -31        0.295455            0.103896   \n",
       "2               13             -10        0.130435            0.048729   \n",
       "3               19              -2        0.809524            0.075000   \n",
       "4               45             -28        0.232877            0.119086   \n",
       "..             ...             ...             ...                 ...   \n",
       "95              33             -24        0.157895            0.087156   \n",
       "96              23              -6        0.586207            0.057426   \n",
       "97              27             -21        0.125000            0.087591   \n",
       "98              27             -21        0.125000            0.087591   \n",
       "99              37             -11        0.541667            0.166667   \n",
       "\n",
       "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0              8.885417                    50.705882  22.126485   \n",
       "1              9.964706                    48.996458  21.782689   \n",
       "2             12.200000                    32.415254  15.932978   \n",
       "3             11.791667                    45.714286  20.134452   \n",
       "4              8.661972                    36.704731  16.757334   \n",
       "..                  ...                          ...        ...   \n",
       "95            11.135593                    46.024465  20.040687   \n",
       "96             6.405063                    36.633663  15.013097   \n",
       "97            12.108696                    48.357664  19.357823   \n",
       "98            12.108696                    48.357664  19.357823   \n",
       "99            11.520000                    57.638889  23.553542   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                          17.653061                 454         850   \n",
       "1                          20.000000                 451         847   \n",
       "2                          24.395349                 170         472   \n",
       "3                          23.375000                 140         280   \n",
       "4                          17.231707                 254         613   \n",
       "..                               ...                 ...         ...   \n",
       "95                         19.603175                 323         654   \n",
       "96                         15.468354                 187         505   \n",
       "97                         22.653061                 277         548   \n",
       "98                         22.653061                 277         548   \n",
       "99                         24.480000                 181         288   \n",
       "\n",
       "    SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0            1.859873                  5         5.466705  \n",
       "1            1.915294                  1         5.539412  \n",
       "2            1.678295                  3         4.934109  \n",
       "3            1.869176                  1         5.439068  \n",
       "4            1.696669                  4         5.007087  \n",
       "..                ...                ...              ...  \n",
       "95           1.908279                  7         5.635552  \n",
       "96           1.670762                 11         4.708436  \n",
       "97           1.865455                  7         5.520909  \n",
       "98           1.865455                  7         5.520909  \n",
       "99           1.993464                  3         5.553922  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_excel(\"data.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
